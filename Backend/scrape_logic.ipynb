{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd7d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "from textblob import TextBlob  \n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "url: str = os.getenv(\"SUPABASE_URL\")\n",
    "key: str = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "def scrape_articles():\n",
    "    # Set up headless Chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # Load search results\n",
    "    driver.get(\"https://u.today/search/node?keys=bitcoin\")\n",
    "    time.sleep(3)  # let JS render\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='news__item')\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_tag = article.find('div', class_='news__item-title')\n",
    "            a_tag = title_tag.find_parent('a') if title_tag else None\n",
    "            href = a_tag['href'] if a_tag and 'href' in a_tag.attrs else None\n",
    "            link = href if href and href.startswith(\"http\") else f\"https://u.today{href}\" if href else None\n",
    "\n",
    "            if not link:\n",
    "                continue\n",
    "\n",
    "            # Fetch article page\n",
    "            response = requests.get(link)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            article_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract details\n",
    "            h1_tag = article_soup.find('h1', class_='article__title')\n",
    "            article_title = h1_tag.get_text(strip=True) if h1_tag else 'N/A'\n",
    "\n",
    "            # Extract date from the correct class\n",
    "            date_tag = article_soup.find('div', class_='article__short-date')\n",
    "            raw_date = date_tag.get_text(strip=True) if date_tag else None\n",
    "            article_datetime = None\n",
    "            if raw_date:\n",
    "                try:\n",
    "                    # Always parse as day/month/year\n",
    "                    article_datetime = pd.to_datetime(raw_date, errors=\"coerce\", dayfirst=True)\n",
    "                    if not pd.isnull(article_datetime):\n",
    "                        article_datetime = article_datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                    else:\n",
    "                        article_datetime = None\n",
    "                except Exception:\n",
    "                    article_datetime = None\n",
    "\n",
    "            author_tag = article_soup.find('div', class_='author-brief__name')\n",
    "            author_name = author_tag.get_text(strip=True) if author_tag else 'N/A'\n",
    "            # Remove leading 'by' if present\n",
    "            if author_name.lower().startswith('by'):\n",
    "                author_name = author_name[2:].strip()\n",
    "\n",
    "            # Extract and combine all <p dir=\"ltr\"> tags for full article content\n",
    "            p_tags = article_soup.find_all('p', attrs={'dir': 'ltr'})\n",
    "            article_text = '\\n'.join([p.get_text(strip=True) for p in p_tags]) if p_tags else 'N/A'\n",
    "\n",
    "            # Sentiment analysis\n",
    "            sentiment = get_sentiment_label(article_text)\n",
    "\n",
    "            # Store result\n",
    "            results.append({\n",
    "                'title': article_title,\n",
    "                'datetime': article_datetime,\n",
    "                'author': author_name,\n",
    "                'link': link,\n",
    "                'content': article_text,\n",
    "                'sentiment': sentiment\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "# After scraping, all datetimes are now in ISO format (YYYY-MM-DDTHH:MM:SS)\n",
    "df_articles = scrape_articles()\n",
    "\n",
    "def get_sentiment_label(text):\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        if polarity > 0.1:\n",
    "            return 'positive'\n",
    "        elif polarity < -0.1:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    except Exception:\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "# Parse both to datetime, handling custom formats in your DataFrame\n",
    "df_articles[\"datetime_parsed\"] = pd.to_datetime(\n",
    "    df_articles[\"datetime\"],\n",
    "    errors=\"coerce\",\n",
    "    dayfirst=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66b5079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last open_time in DB: 2025-08-06T07:45:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "url: str = os.getenv(\"SUPABASE_URL\")\n",
    "key: str = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "binance_endpoint = os.getenv(\"BINANCE_ENDPOINT\")\n",
    "\n",
    "# --- Get the latest open_time from Supabase value table ---\n",
    "latest_value = supabase.table(\"value\").select(\"open_time\").order(\"open_time\", desc=True).limit(1).execute()\n",
    "if latest_value.data and latest_value.data[0].get(\"open_time\"):\n",
    "    last_open_time = latest_value.data[0][\"open_time\"]\n",
    "    print(f\"Last open_time in DB: {last_open_time}\")\n",
    "    # Convert to ms timestamp for Binance API\n",
    "    last_open_time_dt = pd.to_datetime(last_open_time)\n",
    "    last_open_time_ms = int(last_open_time_dt.timestamp() * 1000)\n",
    "    params = {\"startTime\": last_open_time_ms}\n",
    "else:\n",
    "    print(\"No previous value data found, fetching all available data\")\n",
    "    params = {}\n",
    "\n",
    "# Fetch Bitcoin value data from Binance (with startTime if available)\n",
    "response = requests.get(binance_endpoint, params=params)\n",
    "data = response.json()\n",
    "\n",
    "columns = [\n",
    "    \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"close_time\",\n",
    "    \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\",\n",
    "    \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    " ]\n",
    "\n",
    "df_value = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Convert UNIX timestamps to datetime\n",
    "df_value[\"open_time\"] = pd.to_datetime(df_value[\"open_time\"], unit=\"ms\")\n",
    "df_value[\"close_time\"] = pd.to_datetime(df_value[\"close_time\"], unit=\"ms\")\n",
    "\n",
    "# --- Filter out already-inserted open_time values to avoid duplicates ---\n",
    "# Fetch all existing open_time values from Supabase and normalize to UTC\n",
    "existing = supabase.table(\"value\").select(\"open_time\").execute()\n",
    "existing_open_times = set(\n",
    "    pd.to_datetime([row[\"open_time\"] for row in existing.data], utc=True)\n",
    ")\n",
    "\n",
    "# Ensure df_value[\"open_time\"] is also UTC and tz-aware\n",
    "df_value[\"open_time\"] = pd.to_datetime(df_value[\"open_time\"], utc=True)\n",
    "\n",
    "# Filter out already-inserted open_time values\n",
    "df_value = df_value[~df_value[\"open_time\"].isin(existing_open_times)]\n",
    "df_value.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d869582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://postgres.dandiahmbcobmlwsjzrx:D2QTRMHVwRgCNffz@aws-0-eu-central-1.pooler.supabase.com:5432/postgres\n",
      "✅ Tables 'value' and 'articles' ensured.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def create_tables_if_not_exists():\n",
    "    load_dotenv()\n",
    "    conn_str = os.getenv(\"DB_CONN\")\n",
    "    print(conn_str)\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(conn_str) as conn:\n",
    "            with conn.cursor() as cur: \n",
    "                create_value_table = '''\n",
    "                    CREATE TABLE IF NOT EXISTS value (\n",
    "                        open_time TIMESTAMP WITH TIME ZONE PRIMARY KEY,\n",
    "                        open NUMERIC,\n",
    "                        high NUMERIC,\n",
    "                        low NUMERIC,\n",
    "                        close NUMERIC,\n",
    "                        volume NUMERIC,\n",
    "                        close_time TIMESTAMP WITH TIME ZONE,\n",
    "                        quote_asset_volume NUMERIC,\n",
    "                        number_of_trades INTEGER,\n",
    "                        taker_buy_base_asset_volume NUMERIC,\n",
    "                        taker_buy_quote_asset_volume NUMERIC,\n",
    "                        ignore TEXT\n",
    "                    );\n",
    "                    '''\n",
    "                create_articles_table = '''\n",
    "                    CREATE TABLE IF NOT EXISTS articles (\n",
    "                        id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "                        title TEXT,\n",
    "                        link TEXT,\n",
    "                        author TEXT,\n",
    "                        datetime TIMESTAMP WITH TIME ZONE,\n",
    "                        content TEXT,\n",
    "                        sentiment TEXT\n",
    "                    );\n",
    "                    '''\n",
    "                cur.execute(create_value_table)\n",
    "                cur.execute(create_articles_table)\n",
    "                conn.commit()\n",
    "            print(\"✅ Tables 'value' and 'articles' ensured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating tables: {e}\")\n",
    "    conn.close()\n",
    "\n",
    "# Call this function before inserting data\n",
    "create_tables_if_not_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22266c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No new articles to insert.\n",
      "✅ Successfully inserted 8 value rows.\n"
     ]
    }
   ],
   "source": [
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Load .env and connect to Supabase ---\n",
    "load_dotenv()\n",
    "url: str = os.getenv(\"SUPABASE_URL\")\n",
    "key: str = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# --- Step 2: Deduplicate articles before insertion ---\n",
    "def norm_title(t): return t.strip().lower() if isinstance(t, str) else t\n",
    "def norm_dt(dt):\n",
    "    try:\n",
    "        ts = pd.to_datetime(dt, errors=\"coerce\")\n",
    "        if pd.isnull(ts):\n",
    "            ts = pd.to_datetime(dt, errors=\"coerce\", dayfirst=True)\n",
    "        if not pd.isnull(ts):\n",
    "            return ts.tz_localize(None).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Fetch all existing articles from Supabase for deduplication\n",
    "all_existing = []\n",
    "page = 0\n",
    "page_size = 1000\n",
    "while True:\n",
    "    resp = supabase.table(\"articles\").select(\"title\", \"datetime\").range(page*page_size, (page+1)*page_size-1).execute()\n",
    "    if not resp.data:\n",
    "        break\n",
    "    all_existing.extend(resp.data)\n",
    "    if len(resp.data) < page_size:\n",
    "        break\n",
    "    page += 1\n",
    "\n",
    "existing_articles = set(\n",
    "    (norm_title(row[\"title\"]), norm_dt(row[\"datetime\"]))\n",
    "    for row in all_existing if row.get(\"title\") and row.get(\"datetime\")\n",
    ")\n",
    "\n",
    "df_articles[\"title_norm\"] = df_articles[\"title\"].apply(norm_title)\n",
    "df_articles[\"datetime_norm\"] = df_articles[\"datetime\"].apply(norm_dt)\n",
    "\n",
    "df_articles_new = df_articles[\n",
    "    ~df_articles.apply(lambda row: (row[\"title_norm\"], row[\"datetime_norm\"]) in existing_articles, axis=1)\n",
    "].copy()\n",
    "\n",
    "# --- Drop helper columns before inserting ---\n",
    "for col in [\"title_norm\", \"datetime_norm\"]:\n",
    "    if col in df_articles_new.columns:\n",
    "        df_articles_new = df_articles_new.drop(columns=[col])\n",
    "\n",
    "# --- Only keep columns that match your Supabase schema ---\n",
    "supabase_columns = [\"title\", \"link\", \"author\", \"datetime\", \"content\", \"sentiment\"]\n",
    "df_articles_new = df_articles_new[[col for col in supabase_columns if col in df_articles_new.columns]]\n",
    "\n",
    "# --- Step 3: Convert DataFrame to list of dicts and clean date values ---\n",
    "articles_list = df_articles_new.to_dict(orient='records')\n",
    "\n",
    "for article in articles_list:\n",
    "    date_val = article.get(\"datetime\")\n",
    "    ts = pd.to_datetime(date_val, errors=\"coerce\")\n",
    "    if not pd.isnull(ts):\n",
    "        article[\"datetime\"] = ts.tz_localize(None).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    else:\n",
    "        article[\"datetime\"] = None\n",
    "\n",
    "\n",
    "# --- Step 4: Insert only new articles into articles table ---\n",
    "try:\n",
    "    if articles_list:\n",
    "        response = supabase.table(\"articles\").insert(articles_list).execute()\n",
    "        if response.data:\n",
    "            print(f\"✅ Successfully inserted {len(response.data)} articles.\")\n",
    "        else:\n",
    "            print(\"⚠️ Insert succeeded but no data was returned.\")\n",
    "    else:\n",
    "        print(\"✅ No new articles to insert.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Bulk insert failed:\\n{e}\")\n",
    "\n",
    "# --- Step 5: Insert Bitcoin value into values table ---\n",
    "existing = supabase.table(\"value\").select(\"open_time\").execute()\n",
    "existing_open_times = set(pd.to_datetime([row[\"open_time\"] for row in existing.data]))\n",
    "df_value = df_value[~df_value[\"open_time\"].isin(existing_open_times)]\n",
    "df_value.reset_index(drop=True, inplace=True)\n",
    "\n",
    "if 'df_value' in globals() and not df_value.empty:\n",
    "    df_value_insert = df_value.copy()\n",
    "    for col in [\"open_time\", \"close_time\"]:\n",
    "        if col in df_value_insert.columns:\n",
    "            df_value_insert[col] = pd.to_datetime(df_value_insert[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    df_value_insert = df_value_insert.where(pd.notnull(df_value_insert), None)\n",
    "    value_list = df_value_insert.to_dict(orient='records')\n",
    "    response = supabase.table(\"value\").insert(value_list).execute()\n",
    "    if response.data:\n",
    "        print(f\"✅ Successfully inserted {len(response.data)} value rows.\")\n",
    "    else:\n",
    "        print(\"⚠️ Value insert succeeded but no data was returned.\")\n",
    "else:\n",
    "    print(\"✅ No value data to insert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
